# 5_final_project
Image caption generation is a challenging task, the main idea is to automatically generate an image description in a natural language, for this project the language is English. An image caption can be a short description of the objects on the image or a more complex interpretation of the objects and their relationships. For instance, on the following figure we can see an example of image caption

Image caption generation can be done using traditional machine learning techniques, these techniques are mostly based on feature methods such as Scale-Invariant Feature Transformation, Local Binary Patterns, however, limitations and complexity to use these methods on big datasets, makes those methods not widely used. 

There are different approach using deep learning to tackle this challenging task, in this project we will focus on Encoder-Decoder frameworks, in our first reviewed paper called ‘Image captioning using Neural Network Model”[13] an encoder-decoder methods is used, it uses a convolutional Neural Network to get the image features and then transfers those features to a Recurrent Neural Network more specifically a long short term memory LSTM that is used to stablish the correct sequence of the words and generate a sentence in English. A recurrent neural network is a class of artificial neural networks where connections between nodes can create a cycle.

A convolutional neural network is a class of neural network, it uses a series of filters or kernels that extract image features by first identifying simple shapes and them groping those shapes to identify more complex image characteristics [9], even though there are still uncertainty on how exactly our brain process the image, is thought that the process is similar in a convolutional neural network. 

In this paper the convolutional neural network used to extract the features is Visual Geometric Group, VGG-16 is a 16 deep neural network, it has blocks of convolutional layers followed by pooling layers, the pooling layer is used in a so-called pooling stage to keep the most significant information for the feature selection and create a down sampled feature map. For the convolutional layers, a 3x3 kernel is used, where it works like a filter and the main objective of this kernel is to identify features in the image, and for the pooling layer a 2x2 kernel is used to reduce the size of the features map, at the end it has a fully connected layer with 4096 neuron that is passed by a 1x1 layer to finalize in a SoftMax activation function with 1000 possible outputs.
Finally, for generating the sequence of words that is the image caption the algorithm used is the Long short term memory LSTM, it is an artificial recurrent neural network that has feedback. LSTM has an input, a cell, an output and a forget gate.

The dataset used in this paper is Flickr 8k, that is a dataset with 8096 images and up to 5 captions for each image. I think one of the drawbacks of this paper implementation was the lack of evaluation of the model, they only present a couple of examples, but they were not clear on what metrics they used to measure the performance of the model, additionally, they only used the VGG-16 as CNN, it is a widely used CNN, however, by the year of implementation they might have also used other CNN’s to get a comparison between different pretrained models. Additionally, CNN architectures such as VGG-16 might have vanishing gradient problems due to their deep architecture, and they do not state those limitations in the paper. Finally, the model was not compared against a benchmark model or human performance level. 


